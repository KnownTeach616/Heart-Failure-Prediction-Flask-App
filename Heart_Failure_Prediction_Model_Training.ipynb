{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already present.\n",
        "# No specific TensorFlow version needed for Gradient Boosting, so this line can be commented out.\n",
        "!pip install tensorflow==2.12.0\n",
        "\n",
        "# Import core data manipulation and machine learning libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Import GradientBoostingClassifier for the new model.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pickle # Used for saving scikit-learn models.\n",
        "import os\n",
        "\n",
        "print(\"Libraries imported successfully! ‚úÖ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMgO3tWZctCe",
        "outputId": "74ceb8fc-db94-4e70-9dca-4272171192c1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.73.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Libraries imported successfully! ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 1: Data Loading ---\n",
        "\n",
        "# Define the path to the dataset CSV file.\n",
        "# Ensure 'heart_failure_clinical_records_dataset (1).csv' is accessible in your Colab environment.\n",
        "file_path = 'heart_failure_clinical_records_dataset (1).csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame.\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"\\nDataset '{file_path}' loaded successfully! üìä\")\n",
        "print(\"First 5 rows of the dataset for initial review:\")\n",
        "print(df.head())\n",
        "print(f\"\\nDataset shape: {df.shape[0]} rows, {df.shape[1]} columns. This indicates the number of samples and features.\")\n",
        "print(\"\\nDataset information (including data types and non-null counts):\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuS02FlcyWx",
        "outputId": "536da22d-f3f4-45dc-9cd2-b9df7e2f6266"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset 'heart_failure_clinical_records_dataset (1).csv' loaded successfully! üìä\n",
            "First 5 rows of the dataset for initial review:\n",
            "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
            "0  75.0        0                       582         0                 20   \n",
            "1  55.0        0                      7861         0                 38   \n",
            "2  65.0        0                       146         0                 20   \n",
            "3  50.0        1                       111         0                 20   \n",
            "4  65.0        1                       160         1                 20   \n",
            "\n",
            "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
            "0                    1  265000.00               1.9           130    1   \n",
            "1                    0  263358.03               1.1           136    1   \n",
            "2                    0  162000.00               1.3           129    1   \n",
            "3                    0  210000.00               1.9           137    1   \n",
            "4                    0  327000.00               2.7           116    0   \n",
            "\n",
            "   smoking  time  DEATH_EVENT  \n",
            "0        0     4            1  \n",
            "1        0     6            1  \n",
            "2        1     7            1  \n",
            "3        0     7            1  \n",
            "4        0     8            1  \n",
            "\n",
            "Dataset shape: 299 rows, 13 columns. This indicates the number of samples and features.\n",
            "\n",
            "Dataset information (including data types and non-null counts):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 299 entries, 0 to 298\n",
            "Data columns (total 13 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   age                       299 non-null    float64\n",
            " 1   anaemia                   299 non-null    int64  \n",
            " 2   creatinine_phosphokinase  299 non-null    int64  \n",
            " 3   diabetes                  299 non-null    int64  \n",
            " 4   ejection_fraction         299 non-null    int64  \n",
            " 5   high_blood_pressure       299 non-null    int64  \n",
            " 6   platelets                 299 non-null    float64\n",
            " 7   serum_creatinine          299 non-null    float64\n",
            " 8   serum_sodium              299 non-null    int64  \n",
            " 9   sex                       299 non-null    int64  \n",
            " 10  smoking                   299 non-null    int64  \n",
            " 11  time                      299 non-null    int64  \n",
            " 12  DEATH_EVENT               299 non-null    int64  \n",
            "dtypes: float64(3), int64(10)\n",
            "memory usage: 30.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 2: Data Preprocessing ---\n",
        "\n",
        "# Separate features (independent variables) and the target variable (dependent variable).\n",
        "# 'DEATH_EVENT' is the target, indicating whether a heart failure event occurred (1) or not (0).\n",
        "X = df.drop('DEATH_EVENT', axis=1) # Features: all columns except 'DEATH_EVENT'.\n",
        "y = df['DEATH_EVENT'] # Target: the 'DEATH_EVENT' column.\n",
        "\n",
        "print(\"\\nFeatures (X) and Target (y) separated. ‚ú®\")\n",
        "print(f\"Shape of X (features): {X.shape}\")\n",
        "print(f\"Shape of y (target): {y.shape}\")\n",
        "\n",
        "# Split the dataset into training and testing sets.\n",
        "# test_size=0.2: 20% of data for testing, 80% for training.\n",
        "# random_state=42: Ensures reproducibility of the split for consistent results.\n",
        "# stratify=y: Maintains the same proportion of 'DEATH_EVENT' outcomes in both training and testing sets, crucial for imbalanced datasets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nData split into training and testing sets: ‚û°Ô∏è\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Initialize the StandardScaler.\n",
        "# This scales features to have a mean of 0 and a standard deviation of 1, which benefits many ML algorithms.\n",
        "scaler = StandardScaler()\n",
        "print(\"\\nStandardScaler initialized. ‚öôÔ∏è\")\n",
        "\n",
        "# Fit the scaler on the training data and transform it.\n",
        "# The scaler learns the scaling parameters ONLY from the training data to prevent data leakage.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "print(\"\\nX_train data scaled. ‚úÖ\")\n",
        "print(f\"Mean of X_train_scaled (should be close to 0): {np.mean(X_train_scaled):.4f}\")\n",
        "print(f\"Standard Deviation of X_train_scaled (should be close to 1): {np.std(X_train_scaled):.4f}\")\n",
        "print(\"First 5 rows of scaled X_train (example of transformed data):\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "# Transform the test data using the *same* scaler fitted on the training data.\n",
        "# This ensures consistency in scaling between training and inference.\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"\\nX_test data scaled using the same scaler. ‚úÖ\")\n",
        "print(\"First 5 rows of scaled X_test (example of transformed data):\")\n",
        "print(X_test_scaled[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCCgXy1Zc0_c",
        "outputId": "d70e6979-dea2-426f-f6d6-8c728ea25577"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features (X) and Target (y) separated. ‚ú®\n",
            "Shape of X (features): (299, 12)\n",
            "Shape of y (target): (299,)\n",
            "\n",
            "Data split into training and testing sets: ‚û°Ô∏è\n",
            "X_train shape: (239, 12), y_train shape: (239,)\n",
            "X_test shape: (60, 12), y_test shape: (60,)\n",
            "\n",
            "StandardScaler initialized. ‚öôÔ∏è\n",
            "\n",
            "X_train data scaled. ‚úÖ\n",
            "Mean of X_train_scaled (should be close to 0): 0.0000\n",
            "Standard Deviation of X_train_scaled (should be close to 1): 1.0000\n",
            "First 5 rows of scaled X_train (example of transformed data):\n",
            "[[-0.26905031  1.11069566 -0.20073472 -0.90033664  0.17652783 -0.77028133\n",
            "  -1.00472172 -0.36043709  0.55991522 -1.33381774 -0.68283063 -0.46784708]\n",
            " [-0.70688258 -0.90033664 -0.53431791  1.11069566  1.84742492 -0.77028133\n",
            "   1.0516855  -0.54446714 -0.34580213  0.74972762 -0.68283063 -1.35916712]\n",
            " [ 1.2195794  -0.90033664 -0.02058    -0.90033664 -1.49436926  1.29822697\n",
            "   0.01340146  0.46769812 -1.47794881  0.74972762 -0.68283063 -1.59168539]\n",
            " [ 0.25634841 -0.90033664 -0.45512902 -0.90033664 -1.07664499 -0.77028133\n",
            "  -0.17812666  0.92777324 -0.34580213  0.74972762 -0.68283063  1.12102777]\n",
            " [-1.40741421 -0.90033664 -0.02058    -0.90033664 -1.49436926  1.29822697\n",
            "  -1.38777797  0.19165305 -0.34580213  0.74972762 -0.68283063  0.68182659]]\n",
            "\n",
            "X_test data scaled using the same scaler. ‚úÖ\n",
            "First 5 rows of scaled X_test (example of transformed data):\n",
            "[[-0.0939174   1.11069566 -0.34228486  1.11069566 -0.65892071 -0.77028133\n",
            "  -1.14584771 -0.36043709  0.10705655  0.74972762  1.46449201  1.5214759 ]\n",
            " [-0.0939174  -0.90033664  1.64139679 -0.90033664 -0.24119644  1.29822697\n",
            "  -0.35957436 -0.45245212 -0.11937279  0.74972762 -0.68283063 -0.15782272]\n",
            " [-0.0939174   1.11069566  0.13284847 -0.90033664  1.84742492  1.29822697\n",
            "  -0.54102205  0.09963803 -0.34580213  0.74972762  1.46449201 -0.41617636]\n",
            " [-0.26905031  1.11069566 -0.54025708 -0.90033664 -1.07664499 -0.77028133\n",
            "  -0.75271103 -0.08439202 -1.02509014  0.74972762  1.46449201  1.00476863]\n",
            " [-1.58254712  1.11069566 -0.24230889 -0.90033664  1.01197638 -0.77028133\n",
            "  -0.26885051 -0.08439202 -0.34580213 -1.33381774 -0.68283063 -0.39034099]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Model Definition (Gradient Boosting Classifier) ---\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier.\n",
        "# Gradient Boosting builds an ensemble of weak prediction models (typically decision trees) sequentially.\n",
        "# Each new tree corrects the errors made by previously built trees.\n",
        "# n_estimators: The number of boosting stages (trees) to perform. More estimators can improve performance but increase training time and risk overfitting.\n",
        "# learning_rate: Shrinks the contribution of each tree. A lower learning rate often requires more estimators.\n",
        "# max_depth: The maximum depth of the individual regression estimators. Controls the complexity of each tree.\n",
        "# random_state: Ensures reproducibility of the results.\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "print(\"\\nGradient Boosting Classifier model defined. üå≥\")\n",
        "print(\"Initial model parameters:\")\n",
        "print(model.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4dW37xWc56T",
        "outputId": "e8443946-61dc-42a5-e49e-2c4a50b9a06c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Boosting Classifier model defined. üå≥\n",
            "Initial model parameters:\n",
            "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Model Training (Gradient Boosting Classifier) ---\n",
        "\n",
        "print(\"\\nTraining the Gradient Boosting Classifier... üöÄ\")\n",
        "# Train the model using the scaled training data.\n",
        "# For scikit-learn models, training is done directly with the .fit() method.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nModel training complete. üéâ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2qDCC4ic-gL",
        "outputId": "59532f47-3097-459e-844b-ca3e61ff318b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the Gradient Boosting Classifier... üöÄ\n",
            "\n",
            "Model training complete. üéâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 5: Model Evaluation (Gradient Boosting Classifier) ---\n",
        "\n",
        "print(\"\\nEvaluating model on the test set... üîç\")\n",
        "# Make predictions on the scaled test set.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the accuracy score.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print a detailed classification report including precision, recall, and F1-score.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix for a breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM05M5njdA8b",
        "outputId": "68291f2b-4eaa-4b14-b3ae-ada5a1aeb40c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on the test set... üîç\n",
            "Test Accuracy: 0.8333\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88        41\n",
            "           1       0.80      0.63      0.71        19\n",
            "\n",
            "    accuracy                           0.83        60\n",
            "   macro avg       0.82      0.78      0.79        60\n",
            "weighted avg       0.83      0.83      0.83        60\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[38  3]\n",
            " [ 7 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 6: Conditional Model and Scaler Saving (Gradient Boosting Classifier) ---\n",
        "\n",
        "# This block checks if the Test Accuracy is above 80% (0.80).\n",
        "# The model and scaler are saved ONLY if this condition is met, as per project requirements.\n",
        "if accuracy > 0.80:\n",
        "    # Define filenames for the trained Gradient Boosting model and the StandardScaler.\n",
        "    # Scikit-learn models are typically saved as .pkl files.\n",
        "    model_filename = 'gradient_boosting_model.pkl' # Changed filename\n",
        "    scaler_filename = 'scaler.pkl'\n",
        "\n",
        "    # Save the trained Gradient Boosting model using pickle.\n",
        "    # Pickle serializes the Python object (the model) into a byte stream.\n",
        "    with open(model_filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "    print(f\"\\nModel saved successfully as '{model_filename}'! ‚úÖ\")\n",
        "\n",
        "    # Save the StandardScaler using pickle.\n",
        "    with open(scaler_filename, 'wb') as file:\n",
        "        pickle.dump(scaler, file)\n",
        "    print(f\"Scaler saved successfully as '{scaler_filename}'! ‚úÖ\")\n",
        "\n",
        "    # Instructions for downloading the files from Google Colab's file browser.\n",
        "    print(\"\\nTo download these files from Google Colab:\")\n",
        "    print(f\"1. Go to the 'Files' icon on the left sidebar (folder icon).\")\n",
        "    print(f\"2. Locate '{model_filename}' and '{scaler_filename}' in the file list.\")\n",
        "    print(f\"3. Click the three dots (‚ãÆ) next to each file and select 'Download'.\")\n",
        "    print(\"Remember to replace your old files in your local project with these new ones.\")\n",
        "else:\n",
        "    # This message is printed if the accuracy requirement is not met, explicitly preventing saving.\n",
        "    print(\"\\nTest Accuracy is not yet over 80%. ‚ùå Please re-run the training (Section 4) after adjusting parameters, or consider further model tuning (e.g., n_estimators, learning_rate) before saving.\")\n",
        "    print(\"No model or scaler files were saved as the accuracy target was not met.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ezx8SqdJyI",
        "outputId": "d5500516-20ba-4497-f4a2-75ead0006226"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved successfully as 'gradient_boosting_model.pkl'! ‚úÖ\n",
            "Scaler saved successfully as 'scaler.pkl'! ‚úÖ\n",
            "\n",
            "To download these files from Google Colab:\n",
            "1. Go to the 'Files' icon on the left sidebar (folder icon).\n",
            "2. Locate 'gradient_boosting_model.pkl' and 'scaler.pkl' in the file list.\n",
            "3. Click the three dots (‚ãÆ) next to each file and select 'Download'.\n",
            "Remember to replace your old files in your local project with these new ones.\n"
          ]
        }
      ]
    }
  ]
}