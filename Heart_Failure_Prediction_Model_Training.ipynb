{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already present.\n",
        "# No specific TensorFlow version needed for Gradient Boosting, so this line can be commented out.\n",
        "!pip install tensorflow==2.12.0\n",
        "\n",
        "# Import core data manipulation and machine learning libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Import GradientBoostingClassifier for the new model.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pickle # Used for saving scikit-learn models.\n",
        "import os\n",
        "\n",
        "print(\"Libraries imported successfully! ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMgO3tWZctCe",
        "outputId": "74ceb8fc-db94-4e70-9dca-4272171192c1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.73.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Libraries imported successfully! ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 1: Data Loading ---\n",
        "\n",
        "# Define the path to the dataset CSV file.\n",
        "# Ensure 'heart_failure_clinical_records_dataset (1).csv' is accessible in your Colab environment.\n",
        "file_path = 'heart_failure_clinical_records_dataset (1).csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame.\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"\\nDataset '{file_path}' loaded successfully! 📊\")\n",
        "print(\"First 5 rows of the dataset for initial review:\")\n",
        "print(df.head())\n",
        "print(f\"\\nDataset shape: {df.shape[0]} rows, {df.shape[1]} columns. This indicates the number of samples and features.\")\n",
        "print(\"\\nDataset information (including data types and non-null counts):\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuS02FlcyWx",
        "outputId": "536da22d-f3f4-45dc-9cd2-b9df7e2f6266"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset 'heart_failure_clinical_records_dataset (1).csv' loaded successfully! 📊\n",
            "First 5 rows of the dataset for initial review:\n",
            "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
            "0  75.0        0                       582         0                 20   \n",
            "1  55.0        0                      7861         0                 38   \n",
            "2  65.0        0                       146         0                 20   \n",
            "3  50.0        1                       111         0                 20   \n",
            "4  65.0        1                       160         1                 20   \n",
            "\n",
            "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
            "0                    1  265000.00               1.9           130    1   \n",
            "1                    0  263358.03               1.1           136    1   \n",
            "2                    0  162000.00               1.3           129    1   \n",
            "3                    0  210000.00               1.9           137    1   \n",
            "4                    0  327000.00               2.7           116    0   \n",
            "\n",
            "   smoking  time  DEATH_EVENT  \n",
            "0        0     4            1  \n",
            "1        0     6            1  \n",
            "2        1     7            1  \n",
            "3        0     7            1  \n",
            "4        0     8            1  \n",
            "\n",
            "Dataset shape: 299 rows, 13 columns. This indicates the number of samples and features.\n",
            "\n",
            "Dataset information (including data types and non-null counts):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 299 entries, 0 to 298\n",
            "Data columns (total 13 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   age                       299 non-null    float64\n",
            " 1   anaemia                   299 non-null    int64  \n",
            " 2   creatinine_phosphokinase  299 non-null    int64  \n",
            " 3   diabetes                  299 non-null    int64  \n",
            " 4   ejection_fraction         299 non-null    int64  \n",
            " 5   high_blood_pressure       299 non-null    int64  \n",
            " 6   platelets                 299 non-null    float64\n",
            " 7   serum_creatinine          299 non-null    float64\n",
            " 8   serum_sodium              299 non-null    int64  \n",
            " 9   sex                       299 non-null    int64  \n",
            " 10  smoking                   299 non-null    int64  \n",
            " 11  time                      299 non-null    int64  \n",
            " 12  DEATH_EVENT               299 non-null    int64  \n",
            "dtypes: float64(3), int64(10)\n",
            "memory usage: 30.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 2: Data Preprocessing ---\n",
        "\n",
        "# Separate features (independent variables) and the target variable (dependent variable).\n",
        "# 'DEATH_EVENT' is the target, indicating whether a heart failure event occurred (1) or not (0).\n",
        "X = df.drop('DEATH_EVENT', axis=1) # Features: all columns except 'DEATH_EVENT'.\n",
        "y = df['DEATH_EVENT'] # Target: the 'DEATH_EVENT' column.\n",
        "\n",
        "print(\"\\nFeatures (X) and Target (y) separated. ✨\")\n",
        "print(f\"Shape of X (features): {X.shape}\")\n",
        "print(f\"Shape of y (target): {y.shape}\")\n",
        "\n",
        "# Split the dataset into training and testing sets.\n",
        "# test_size=0.2: 20% of data for testing, 80% for training.\n",
        "# random_state=42: Ensures reproducibility of the split for consistent results.\n",
        "# stratify=y: Maintains the same proportion of 'DEATH_EVENT' outcomes in both training and testing sets, crucial for imbalanced datasets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nData split into training and testing sets: ➡️\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Initialize the StandardScaler.\n",
        "# This scales features to have a mean of 0 and a standard deviation of 1, which benefits many ML algorithms.\n",
        "scaler = StandardScaler()\n",
        "print(\"\\nStandardScaler initialized. ⚙️\")\n",
        "\n",
        "# Fit the scaler on the training data and transform it.\n",
        "# The scaler learns the scaling parameters ONLY from the training data to prevent data leakage.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "print(\"\\nX_train data scaled. ✅\")\n",
        "print(f\"Mean of X_train_scaled (should be close to 0): {np.mean(X_train_scaled):.4f}\")\n",
        "print(f\"Standard Deviation of X_train_scaled (should be close to 1): {np.std(X_train_scaled):.4f}\")\n",
        "print(\"First 5 rows of scaled X_train (example of transformed data):\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "# Transform the test data using the *same* scaler fitted on the training data.\n",
        "# This ensures consistency in scaling between training and inference.\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"\\nX_test data scaled using the same scaler. ✅\")\n",
        "print(\"First 5 rows of scaled X_test (example of transformed data):\")\n",
        "print(X_test_scaled[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCCgXy1Zc0_c",
        "outputId": "d70e6979-dea2-426f-f6d6-8c728ea25577"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features (X) and Target (y) separated. ✨\n",
            "Shape of X (features): (299, 12)\n",
            "Shape of y (target): (299,)\n",
            "\n",
            "Data split into training and testing sets: ➡️\n",
            "X_train shape: (239, 12), y_train shape: (239,)\n",
            "X_test shape: (60, 12), y_test shape: (60,)\n",
            "\n",
            "StandardScaler initialized. ⚙️\n",
            "\n",
            "X_train data scaled. ✅\n",
            "Mean of X_train_scaled (should be close to 0): 0.0000\n",
            "Standard Deviation of X_train_scaled (should be close to 1): 1.0000\n",
            "First 5 rows of scaled X_train (example of transformed data):\n",
            "[[-0.26905031  1.11069566 -0.20073472 -0.90033664  0.17652783 -0.77028133\n",
            "  -1.00472172 -0.36043709  0.55991522 -1.33381774 -0.68283063 -0.46784708]\n",
            " [-0.70688258 -0.90033664 -0.53431791  1.11069566  1.84742492 -0.77028133\n",
            "   1.0516855  -0.54446714 -0.34580213  0.74972762 -0.68283063 -1.35916712]\n",
            " [ 1.2195794  -0.90033664 -0.02058    -0.90033664 -1.49436926  1.29822697\n",
            "   0.01340146  0.46769812 -1.47794881  0.74972762 -0.68283063 -1.59168539]\n",
            " [ 0.25634841 -0.90033664 -0.45512902 -0.90033664 -1.07664499 -0.77028133\n",
            "  -0.17812666  0.92777324 -0.34580213  0.74972762 -0.68283063  1.12102777]\n",
            " [-1.40741421 -0.90033664 -0.02058    -0.90033664 -1.49436926  1.29822697\n",
            "  -1.38777797  0.19165305 -0.34580213  0.74972762 -0.68283063  0.68182659]]\n",
            "\n",
            "X_test data scaled using the same scaler. ✅\n",
            "First 5 rows of scaled X_test (example of transformed data):\n",
            "[[-0.0939174   1.11069566 -0.34228486  1.11069566 -0.65892071 -0.77028133\n",
            "  -1.14584771 -0.36043709  0.10705655  0.74972762  1.46449201  1.5214759 ]\n",
            " [-0.0939174  -0.90033664  1.64139679 -0.90033664 -0.24119644  1.29822697\n",
            "  -0.35957436 -0.45245212 -0.11937279  0.74972762 -0.68283063 -0.15782272]\n",
            " [-0.0939174   1.11069566  0.13284847 -0.90033664  1.84742492  1.29822697\n",
            "  -0.54102205  0.09963803 -0.34580213  0.74972762  1.46449201 -0.41617636]\n",
            " [-0.26905031  1.11069566 -0.54025708 -0.90033664 -1.07664499 -0.77028133\n",
            "  -0.75271103 -0.08439202 -1.02509014  0.74972762  1.46449201  1.00476863]\n",
            " [-1.58254712  1.11069566 -0.24230889 -0.90033664  1.01197638 -0.77028133\n",
            "  -0.26885051 -0.08439202 -0.34580213 -1.33381774 -0.68283063 -0.39034099]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Model Definition (Gradient Boosting Classifier) ---\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier.\n",
        "# Gradient Boosting builds an ensemble of weak prediction models (typically decision trees) sequentially.\n",
        "# Each new tree corrects the errors made by previously built trees.\n",
        "# n_estimators: The number of boosting stages (trees) to perform. More estimators can improve performance but increase training time and risk overfitting.\n",
        "# learning_rate: Shrinks the contribution of each tree. A lower learning rate often requires more estimators.\n",
        "# max_depth: The maximum depth of the individual regression estimators. Controls the complexity of each tree.\n",
        "# random_state: Ensures reproducibility of the results.\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "print(\"\\nGradient Boosting Classifier model defined. 🌳\")\n",
        "print(\"Initial model parameters:\")\n",
        "print(model.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4dW37xWc56T",
        "outputId": "e8443946-61dc-42a5-e49e-2c4a50b9a06c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Boosting Classifier model defined. 🌳\n",
            "Initial model parameters:\n",
            "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Model Training (Gradient Boosting Classifier) ---\n",
        "\n",
        "print(\"\\nTraining the Gradient Boosting Classifier... 🚀\")\n",
        "# Train the model using the scaled training data.\n",
        "# For scikit-learn models, training is done directly with the .fit() method.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nModel training complete. 🎉\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2qDCC4ic-gL",
        "outputId": "59532f47-3097-459e-844b-ca3e61ff318b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the Gradient Boosting Classifier... 🚀\n",
            "\n",
            "Model training complete. 🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 5: Model Evaluation (Gradient Boosting Classifier) ---\n",
        "\n",
        "print(\"\\nEvaluating model on the test set... 🔍\")\n",
        "# Make predictions on the scaled test set.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the accuracy score.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print a detailed classification report including precision, recall, and F1-score.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix for a breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM05M5njdA8b",
        "outputId": "68291f2b-4eaa-4b14-b3ae-ada5a1aeb40c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on the test set... 🔍\n",
            "Test Accuracy: 0.8333\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88        41\n",
            "           1       0.80      0.63      0.71        19\n",
            "\n",
            "    accuracy                           0.83        60\n",
            "   macro avg       0.82      0.78      0.79        60\n",
            "weighted avg       0.83      0.83      0.83        60\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[38  3]\n",
            " [ 7 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 6: Conditional Model and Scaler Saving (Gradient Boosting Classifier) ---\n",
        "\n",
        "# This block checks if the Test Accuracy is above 80% (0.80).\n",
        "# The model and scaler are saved ONLY if this condition is met, as per project requirements.\n",
        "if accuracy > 0.80:\n",
        "    # Define filenames for the trained Gradient Boosting model and the StandardScaler.\n",
        "    # Scikit-learn models are typically saved as .pkl files.\n",
        "    model_filename = 'gradient_boosting_model.pkl' # Changed filename\n",
        "    scaler_filename = 'scaler.pkl'\n",
        "\n",
        "    # Save the trained Gradient Boosting model using pickle.\n",
        "    # Pickle serializes the Python object (the model) into a byte stream.\n",
        "    with open(model_filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "    print(f\"\\nModel saved successfully as '{model_filename}'! ✅\")\n",
        "\n",
        "    # Save the StandardScaler using pickle.\n",
        "    with open(scaler_filename, 'wb') as file:\n",
        "        pickle.dump(scaler, file)\n",
        "    print(f\"Scaler saved successfully as '{scaler_filename}'! ✅\")\n",
        "\n",
        "    # Instructions for downloading the files from Google Colab's file browser.\n",
        "    print(\"\\nTo download these files from Google Colab:\")\n",
        "    print(f\"1. Go to the 'Files' icon on the left sidebar (folder icon).\")\n",
        "    print(f\"2. Locate '{model_filename}' and '{scaler_filename}' in the file list.\")\n",
        "    print(f\"3. Click the three dots (⋮) next to each file and select 'Download'.\")\n",
        "    print(\"Remember to replace your old files in your local project with these new ones.\")\n",
        "else:\n",
        "    # This message is printed if the accuracy requirement is not met, explicitly preventing saving.\n",
        "    print(\"\\nTest Accuracy is not yet over 80%. ❌ Please re-run the training (Section 4) after adjusting parameters, or consider further model tuning (e.g., n_estimators, learning_rate) before saving.\")\n",
        "    print(\"No model or scaler files were saved as the accuracy target was not met.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ezx8SqdJyI",
        "outputId": "d5500516-20ba-4497-f4a2-75ead0006226"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved successfully as 'gradient_boosting_model.pkl'! ✅\n",
            "Scaler saved successfully as 'scaler.pkl'! ✅\n",
            "\n",
            "To download these files from Google Colab:\n",
            "1. Go to the 'Files' icon on the left sidebar (folder icon).\n",
            "2. Locate 'gradient_boosting_model.pkl' and 'scaler.pkl' in the file list.\n",
            "3. Click the three dots (⋮) next to each file and select 'Download'.\n",
            "Remember to replace your old files in your local project with these new ones.\n"
          ]
        }
      ]
    }
  ]
}